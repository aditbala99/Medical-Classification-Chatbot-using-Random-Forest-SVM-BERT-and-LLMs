{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb04b73-8aa4-4dd9-bb4d-7d1041043895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from transformers import RobertaTokenizer, TFRobertaModel, BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "  \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "  storage_client = storage.Client()\n",
    "  bucket = storage_client.get_bucket(bucket_name)\n",
    "  blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "  blob.upload_from_filename(source_file_name)\n",
    "\n",
    "  print('File {} uploaded to {}.'.format(\n",
    "      source_file_name,\n",
    "      destination_blob_name))\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"Downloads/train.csv\")\n",
    "test_df = pd.read_csv(\"Downloads/test.csv\")\n",
    "validation_df = pd.read_csv(\"Downloads/validation.csv\")\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_data(df):\n",
    "    df['topic_name'].fillna(df['topic_name'].mode()[0], inplace=True)\n",
    "    df = df.dropna(subset=[\"question\", \"exp\"])\n",
    "    mode_values = df[['opa', 'opb', 'opc', 'opd']].mode().iloc[0]\n",
    "    df[['opa', 'opb', 'opc', 'opd']] = df[['opa', 'opb', 'opc', 'opd']].fillna(mode_values)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['cop'] = label_encoder.fit_transform(df['cop'])\n",
    "    return df\n",
    "\n",
    "# Preprocess the data\n",
    "train_df = preprocess_data(train_df)\n",
    "validation_df = preprocess_data(validation_df)\n",
    "\n",
    "# Split the data into features and target\n",
    "X_train = train_df['question']\n",
    "y_cop_train = train_df['cop']\n",
    "y_exp_train = train_df['exp']\n",
    "y_topic_name_train = train_df['topic_name']\n",
    "\n",
    "X_val = validation_df['question']\n",
    "y_cop_val = validation_df['cop']\n",
    "\n",
    "#BERT\n",
    "PRETRAINED_LM = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM, do_lower_case=True)\n",
    "N_labels = len(train_df.cop.unique())\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_LM,\n",
    "                                                      num_labels=N_labels,\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "# Tokenize and encode data for BERT\n",
    "def encode(docs):\n",
    "    encoded_dict = tokenizer.batch_encode_plus(docs, add_special_tokens=True, max_length=128, padding='max_length',\n",
    "                                               return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_masks = encoded_dict['attention_mask']\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "train_input_ids, train_att_masks = encode(X_train.values.tolist())\n",
    "valid_input_ids, valid_att_masks = encode(X_val.values.tolist())\n",
    "\n",
    "# Build DataLoader for BERT\n",
    "BATCH_SIZE = 16\n",
    "train_dataset = TensorDataset(train_input_ids, train_att_masks, torch.LongTensor(y_cop_train.values.tolist()))\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataset = TensorDataset(valid_input_ids, valid_att_masks, torch.LongTensor(y_cop_val.values.tolist()))\n",
    "valid_sampler = SequentialSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Move BERT model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up BERT optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.000005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "\n",
    "\n",
    "# Train BERT model\n",
    "EPOCHS = 15\n",
    "for epoch_num in range(EPOCHS):\n",
    "    print('Epoch: ', epoch_num + 1)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step_num, batch_data in enumerate(tqdm(train_dataloader, desc='Training')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        output = model(input_ids=input_ids, attention_mask=att_mask, labels=labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in tqdm(valid_dataloader, desc='Validation'):\n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "            output = model(input_ids=input_ids, attention_mask=att_mask, labels=labels)\n",
    "            valid_loss += output.loss.item()\n",
    "            valid_pred.append(torch.argmax(output.logits.cpu().detach(), axis=-1))\n",
    "\n",
    "    # Calculate and print losses\n",
    "    train_loss = loss.item()\n",
    "    valid_loss /= len(valid_dataloader)\n",
    "    print(f'Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
